---
layout: post
title: AlexNet 논문리뷰
date: 2024-11-04 06:00:00 +0800
category: paper
thumbnail: /style/image/thumbnail.png
icon: code
---

# AlexNet

## Abstract

- ImageNet LSVRC-2010은 120만개의 고해상도 이미지로 구성된 데이터셋을 사용하여 1000가지 클래스 이미지를 분류하는 대회이다. 이 대회에서 논문의 네트워크는 테스트 데이터로 37.5%의 top-1 오류율과 17.0%의 top-5의 오류율을 달성하여, 이전 네트워크들의 최고 성능을 능가함.
- 빠른 학습을 위하여 GPU를 사용하고, 오버피팅을 줄이기 위해 DropOut을 적용하여 대회에서 15.3%의 top-5 테스트 오류율로 우승을 함

## Introduction

네트워크의 성능을 향상시키기 위해 더 큰 데이터셋을 수집하여 모델을 학습, 오버피팅을 방지하기 위한 기술을 사용

최근까지 레이블이 지정된 이미지 데이터셋은 수만개의 이미지 정도로 작았다, 단순 인식 작업은 이 정도의 크기의 데이터셋으로도 해결할 수 있음.

하지만 현실적인 환경에서의 객체는 상당한 변동성을 갖으므로, 이를 인식하도록 학습하려면 훨씬 더 큰 학습 데이터가 필요로 함.

수백만 개의 이미지에서 수천 개의 객체에 대해 학습하려면 대규모 학습 용량을 가진 모델이 필요.
그럼 모델을 학습에 드는 비용이 엄청 많아지게 된다.
이를 2D 컨볼루션 및 CNN 훈련에 내재된 다른 모든 연산에 대해 고도로 최적화된 GPU 구현을 작성. 이 네트워크는 성능 향상 및 학습 시간 단축을 위한 새롭고 특이한 기능을 포함.

## Dataset

22000개의 범주에 속하는 1500만개 이상의 레이블링된 고해상도 이미지 데이터셋

- 이미지는 웹에서 수집
- Amazon의 Mechanical Turk 크라우드소싱 도구를 사용하여 사람이 라벨링 작업
- 네트워크에 일정한 입력 차원을 위해 256x256 고정 해상도로 다운 샘플링

## The Architecture

### ReLU Nonlinearity

이전의 네트워크는 활성화함수로 Tanh 혹은 Sigmoid 사용했지만 해당 논문에서는 ReLU를 사용
AlexNet이 CNN 역사상 최초로 ReLU를 쓴 논문은 아니지만, 대규모 CNN 학습에서 ReLU의 효율성을 증명.

ReLU를 사용하는 심층 합성곱 신경망은 tanh 유닛을 사용하는 동등한 네트워크보다 몇 배 더 빠르게 학습.
곧, 대규모 데이터셋에서 큰 영향을 미칠 수 있음.

### Training on Multiple GPUs

대회다시 GTX 580 GPU는 3GB의 메모리만 가지고 있어, 학습할 수 있는 네트워크의 크기가 제한됨. 저자들은 그래서 2개의 GPU를 사용함.

두개의 GPU는 호스트 머신 메모리를 거치는 과정을 없애고 직접적으로 읽기와 쓰기를 하기때문에 GPU 간 병렬화에 특히 적합함. 적용한 병렬화 방식은 본질적으로 커널 또는 뉴런의 절반을 각 GPU에 배치하고 GPU들을 특정 계층에서만 통신함. 이 방식은 교차 검증의 문제가 있지만 통신량을 계산량의 허용 가능한 비율이 될 때까지 정확하게 조정할 수 있음.

결론적으로 두개의 GPU를 사용하는 네트워크는 하나의 GPU를 사용하는 네트워크보다 학습 시간이 줄어들게 된다.

### Local Response Normalization

ReLU는 신경망의 활성화 함수가 특정 입력값 범위를 넘어서면 출력이 더 이상 크게 변하지 않고 거의 일정하게 유지되는 현상을 방지하기 위해 입력을 정규화할 필요가 없음. 그러나 저자들은 다음의 지역 정규화 기법이 일반화에 도움이 된다는 것을 발견함.

![스크린샷 2025-12-13 오후 4.21.43.png](style/image/AlexNet_paper/1.png)

이 기법은 Jarrett 등의 지역 대비 정규화 기법과 어느 정도 유사성을 보이지만, 평균 활성값을 빼지 않기 때문에 저자들의 기법은 “brightness normalization”라고 정확하게 명명될 수 있음. top-1및 top-5 오류율을 각각 1.4%및 1.2% 감소시킴. 

### Overlapping Pooling

CNN의 Pooling layer는 동일한 커널 맵 내의 인접한 뉴런들의 output을 요약, 전통적으로 인접한 뉴런들은 겹치지 않게 풀링 레이어를 통과시킨다. 

### Overall Architecture

![image.png](style/image/AlexNet_paper/2.png)

- 전체 아키텍처 설명

네트워크는 5개의 컨볼루션 레이어와 세개의 fully-connected 레이어를 포함하여 8개의 계층으로 구성되어 있다.
마지막 fully-connected 레이어는 이미지넷 대회에 1000개의 클래스에 매치되는 소프트맥스 계층으로 되어있다.

이 네트워크는 다항 로지스틱 회귀 목적 함수를 최대화하며, 이는 예측 분포하에서 올바른 레이블의 로그 확률을 훈련 사례에 대해 평균화하는 것과 같다.

2, 4, 5번째 컨볼루션 레이어의 커널은 이전 레이어의 커널맵 중 동일한 GPU에 있는 것들에만 연결됨. 세번째 레이어는 모든 두번째 레이어에 연결된다. 완전 연결 레이어의 뉴런은 이전 레이어의 모든 뉴런에 연결됨.’

response-normalization 계층은 1, 2번째 컨볼루션 다음에 들어가고, 맥스 풀링 레이어는 response-normalization과 다섯번째 계층 뒤에 들어간다.
활성화 함수 ReLU는 모든 합성곱, fully-connected 계층의 출력에 적용

첫 번째 컨볼루션 레이어는 224 × 224 × 3 입력 이미지를 크기 11 × 11 × 3의 96개 커널로 필터링하며, 스트라이드(stride)는 4픽셀입니다(이는 커널 맵 내의 인접 뉴런의 수용장 중심 간의 거리입니다).

3 픽셀이며, 스트라이드는 4픽셀입니다(이는 커널 맵 내의 인접 뉴런의 수용장 중심 간의 거리입니다).
4 픽셀의 스트라이드를 갖는 3 (이는 커널 맵 내 이웃 뉴런들의 수용장 중심 간의 거리입니다). 두 번째 컨볼루션 계층은 첫 번째 컨볼루션 계층의 (응답 정규화 및 풀링된) 출력을 입력으로 받아 5 × 5 × 48 크기의 256개 커널로 필터링합니다. 세 번째, 네 번째, 다섯 번째 컨볼루션 계층은 중간 풀링 또는 정규화 계층 없이 서로 연결됩니다. 세 번째 컨볼루션 계층은 두 번째 컨볼루션 계층의 (정규화 및 풀링된) 출력에 연결된 3 × 3 × 256 크기의 384개 커널을 가집니다. 네 번째 컨볼루션 계층은 3 × 3 × 192 크기의 384개 커널을 가지며, 다섯 번째 컨볼루션 계층은 3 × 3 × 192 크기의 256개 커널을 가집니다. 완전 연결 계층은 각각 4096개의 뉴런을 가집니다.

### Reducing Overfitting

이 네트워크 아키텍처는 6천만 개의 파라미터를 가짐, 이렇게 많은 파라미터를 오버피팅 없이 학습하기에는 불가능하다 판단하여 데이터 증강과 드롭아웃 시스템을 적용하여 해결하고자 함.

### Details of learning

batch_size = 128개, momentum=0.9, weight_decay=0.0005의 확률적 경가 하강법으로 모델을 학습에 이용.
저자들은 적응 양의 weight decay가 모델 학습에 중요함을 발견했다고 함.
(weight decay는 “모델이 너무 욕심내서 숫자를 크게 키우지 못하게 말리는 장치”라고 할 수 있음)
weight_decay가 단순히 규제 작용을 하는게 아닌 모델 학습에러도 줄여줌.

각 계층의 가중치는 평균이 0이고 표준편차가 0.01인 가우시안 분포에서 초기화했습니다. 두 번째, 네 번째, 다섯 번째 합성곱 계층(convolutional layers) 및 완전 연결 은닉 계층의 뉴런 편향은 상수 1로 초기화했습니다. 이 초기화는 ReLU에 양수 입력을 제공하여 학습 초기 단계를 가속화합니다. 나머지 계층의 뉴런 편향은 상수 0으로 초기화했습니다.

모든 계층에 대해 동일한 학습률을 사용했으며, 훈련 중에 수동으로 조정했습니다. 우리가 따른 휴리스틱은 현재 학습률로 검증 오류율이 더 이상 개선되지 않을 때 학습률을 10으로 나누는 것이었습니다. 학습률은 0.01로 초기화되었고 종료 전에 세 번 감소했습니다.

학습률은 0.01로 초기화되었으며 종료 전에 세 번 감소되었습니다. 우리는 120만 개의 이미지로 구성된 훈련 세트를 통해 약 90주기 동안 네트워크를 훈련했으며, 이는 두 대의 NVIDIA GTX 580 3GB GPU에서 5일에서 6일이 소요되었습니다.

## Results

![image.png](style/image/AlexNet_paper/3.png)

해당 네트워크는 37.5%와 17.0%의 top-1 및 top-5 테스트 세트 오류율을 달성.

## Qualitative Evaluations

![image.png](style/image/AlexNet_paper/4.png)

이 네트워크에서는 다양한 feature을 학습했다(주파수, 방향, 색상 등). 하나의 GPU가 색상 정보가 없지만 다른 GPU가 색상 정보를 담는 식으로 진행.

위 Figure의 왼쪽 섹션은 8개의 테스트 이미지에 대해 모델이 예측한 top-5 클래스이다. 이미지 중 mite와 같은 경우에는 중앙에 객체가 없어서 잘 분류할 수 있었음. top-5의 올라온 항목도 치타와 제규어 cat과 같이 유사한 일리가 있는 예측을 하는 모습을 확인할 수 있었다.

네트워크의 시각적 지식을 탐색하는 또 다른 방법은 마지막 4096차원 은닉층에서 이미지에 의해 유도되는 특징 활성화를 고려하는 것입니다. 두 이미지가 작은 유클리드 거리 분리를 갖는 특징 활성화 벡터를 생성한다면, 신경망의 더 높은 수준에서 이를 유사하다고 간주한다고 말할 수 있습니다. Figure 4는 테스트 세트의 다섯 이미지와 이 측정값에 따라 각 이미지와 가장 유사한 훈련 세트의 여섯 이미지를 보여줍니다. 픽셀 수준에서 검색된 훈련 이미지는 일반적으로 첫 번째 열의 쿼리 이미지에 L2 거리로 가깝지 않다는 점에 주목하십시오. 예를 들어, 검색된 개와 코끼리는 다양한 포즈로 나타납니다. 보충 자료에서 더 많은 테스트 이미지에 대한 결과를 제시합니다.

두 개의 4096차원 실수 벡터 간의 유클리드 거리를 사용하여 유사성을 계산하는 것은 비효율적이지만, 오토인코더를 훈련하여 이러한 벡터를 짧은 이진 코드로 압축하면 효율적으로 만들 수 있습니다. 이는 이미지 레이블을 사용하지 않아 의미론적으로 유사한지 여부에 관계없이 유사한 에지 패턴을 가진 이미지를 검색하는 경향이 있는 원시 픽셀 [14]에 오토인코더를 적용하는 것보다 훨씬 더 나은 이미지 검색 방법을 제공해야 합니다.

## Discussion

저자는 결과는 크고 깊은 컨볼루션 신경망이 순전히 지도 학습을 사용하여 매우 어려운 데이터셋에서 기록적인 결과를 달성할 수 있음을 보여주었고 단일 컨볼루션 레이어가 제거되면 네트워크 성능이 저하된다는 점을 주목할 만 하다고 함.

실험을 단순화하기 위해 비지도 사전 학습을 사용하지 않았지만, 레이블이 지정된 데이터 양의 해당 증가 없이 네트워크 크기를 상당히 늘릴 수 있는 충분한 계산 능력을 확보한다면 특히 도움이 될 것으로 예상합니다. 지금까지 네트워크를 더 크게 만들고 더 오래 훈련하면서 결과가 개선되었지만, 인간 시각 시스템의 추론-시간 경로와 일치시키려면 아직 수십억 배나 더 가야 합니다. 궁극적으로 우리는 정적 이미지에서는 누락되거나 훨씬 덜 명확한 시간 구조가 매우 유용한 정보를 제공하는 비디오 시퀀스에서 매우 크고 깊은 컨볼루션 네트워크를 사용하고 싶습니다.