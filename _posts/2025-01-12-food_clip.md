---
layout: post
title: 식당 추천 (CLIP 기반)
date: 2025-01-11 04:00:00 +0800
category: experiment
thumbnail: /style/image/thumbnail.png
icon: code
---

# 식당 추천

- 이전 Post의 CLIP 실습을 바탕으로 진행

# 🧪 프로젝트 **— Food_CLIP 기반 음식 분류 및 근처 식당 추천 시스템**

| 항목 | 내용 |
| --- | --- |
| **프로젝트명** | Food_CLIP |
| **모델(Model)** | CLIP ViT-B/32 (OpenAI pretrained) |
| **기간** | 2025.01.10 ~ 2025.01.12 |
| **환경** | Windows 10 64bit, NVIDIA RTX 3060, Python 3.10, PyTorch, CUDA 12.x |

---

## 🎯 **목적 (Objective)**

- 단일 음식 이미지 입력만으로 해당 음식 카테고리를 분류하고,
- 지리적 반경(예: 5km) 내에서 유사한 음식을 판매하는 식당을 자동 탐색하는 **멀티모달 기반 음식 추천 시스템** 구현.

---

## 💡 **배경 및 아이디어 (Background & Motivation)**

최근 CLIP(OpenAI)이 제공하는 **이미지-텍스트 조정 임베딩(joint embedding)** 구조가 이미지 검색, OCR, 비전-Language task에서 높은 활용성을 보임.

> “텍스트 설명과 시각적 특징이 같은 공간에서 의미적으로 정렬된다.”
> 

이 프로젝트는 CLIP의 임베딩 특성을 이용:

- 별도의 라벨링 없이
- 음식 이미지를 텍스트 벡터와 비교하여 자동 분류
- 이후 지역 기반 검색 API(Kakao Local API)와 결합하여 추천 시스템 구축

이라는 practical pipeline 검증을 목표로 한다.

---

## 📦 **데이터셋 (Dataset)**

현재 단계에서는 **라벨링 없이 임베딩 비교 방식 사용**.

| 구성 | 내용 |
| --- | --- |
| 입력 이미지 | 사용자 업로드(`png/jpg`) |
| 텍스트 후보 | 사전 정의된 음식 텍스트 리스트 |
| 저장된 파일 | `food_labels.json` (카테고리 메타데이터 포함) |

### JSON 구조 예시

```json
{
  "id": "ramen",
  "caption": "a photo of ramen",
  "ko_keywords": ["라멘", "라면", "일식 라멘"]
}
// 바로 한글로 하게되면 CLIP이 유사도를 제대로 계산 못함 ;
```

---

## ⚙️ **환경 (System & Tools)**

| 항목 | 상세 |
| --- | --- |
| OS | Windows 10 |
| GPU | RTX 3060 12GB |
| CPU | AMD Ryzen 5 5600X |
| Framework | PyTorch 2.x |
| 모델 | open_clip ViT-B/32 |
| API 연동 | Kakao Local Search API |
| 언어 | Python 3.10  |

---

## 🧠 **실험 설계 (Experiment Design)**

### 🔍 학습 방식

- Pre-trained CLIP 사용 (Fine-tuning 없음)
- Zero-shot classification 기반

### 파라미터

| 요소 | 값 |
| --- | --- |
| 임베딩 dim | 512 |
| cosine similarity | 사용 |
| normalizing embedding | Yes |
| Matching 방식 | argmax(similarity_score) |

### 비교 대상

| 후보 텍스트 | 예시 |
| --- | --- |
| a photo of ramen | 🍜 |
| a photo of pizza | 🍕 |
| a plate of sushi | 🍣 |
| ... |  |

---

## 📊 **결과 및 분석 (Results & Observations)**

### 예시 실행 결과

```
분류 이미지: /images/ramen.png (흰 배경에 라멘만 있는 이미지)

1) ramen (score=0.312)
2) soba (score=0.290)
3) bibimbap (score=0.268)

```

→ 음식 사진이 라멘이었으며, CLIP은 ramen과 soba를 구분하는 semantic 특성 반영.

### 지역 검색 결과

- kakao map API를 사용
- input image와 가장 유사한 단어를 입력으로 사용
- 사용자의 현재 위치를 반영하여 반경 5km 내의 식당 검색

```
사용자 위치 반경 5km 조건 적용
검색된 식당 수: 12

예시:
- 키와마루아지 용인명지대점 (1756m)
- 킨토토 역북점 (1700m)
- 고칸 용인점 (1835m)

```

---

## 🔍 인사이트 (Insights & Conclusion)

- CLIP 모델만으로도 **food zero-shot classification**이 충분히 실용성 있는 성능을 보였다.
- 위치 기반 필터링을 결합하여 실제 음식점 추천 서비스 형태까지 확장 가능함.
- Top-3 ranking을 보면 음식 도메인에서의 semantic similarity가 잘 반영됨.

---

## 🚀 향후 개선 방향 (Next Steps)

| 개선 방향 | 설명 |
| --- | --- |
| 🔥 FAISS 기반 Embedding Index 구축 | 데이터 수가 증가하면 현재 방식은 O(N). FAISS로 빠른 검색 구현 필요 |
| 🧠 Category 확장 | food_labels.json → 200+ 음식 라벨링 |
| 🏗 Embedding DB 구축 | Vector DB (Pinecone, Qdrant, Milvus) 후보 |
| 👀 UI기능 확장 | Streamlit, FastAPI 기반 Web Demo |
| 🧪 Fine-tuning | Food-101 dataset으로 CLIP adapter 기반 fine-tuning |

---

## 🔗 참고 자료

- 프로젝트 코드: [Clip-based-restaurant-recommendation](https://github.com/moveright1231/Clip-based-restaurant-recommendation)
